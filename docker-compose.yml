services:
  # Ollama Service
  ollama:
    image: ollama/ollama:0.12.0
    pull_policy: if_not_present
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama/ollama.json:/root/.ollama/ollama.json
    networks:
      - ollama_network
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_DEBUG=1
      - OLLAMA_KEEP_ALIVE=60m
      - OLLAMA_VERBOSE=1
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_NUM_CTX=16384
      - OLLAMA_NUM_THREAD=8
      - OLLAMA_MLOCK=false
      - OLLAMA_NUM_BATCH=512  # Higher value for better performance
      - OLLAMA_MAX_LOADED_MODELS=1  # Only one model in VRAM at a time
      - OLLAMA_MAX_QUEUE=20  # Allow up to 20 queued requests
      - OLLAMA_NEW_ESTIMATES=1  # Enable improved memory management
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
          memory: 16G
        limits:
          memory: 36G
    shm_size: 4gb  # Increase from default 64MB to 1GB
    oom_kill_disable: true
    command: serve
    ulimits:
      memlock: -1  # Unlimited locked memory    

  proxy:
    build: .
    restart: unless-stopped
    environment:
      - API_KEY=${API_KEY}
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}
    depends_on:
      - ollama
    networks:
      - ollama_network

  # Cloudflared Tunnel
  cloudflared:
    image: cloudflare/cloudflared:latest
    restart: unless-stopped
    networks:
      - ollama_network
    volumes:
      - ./cloudflare:/etc/cloudflared
    command: tunnel --no-autoupdate run
    depends_on:
      - proxy

  # GPU Watchdog Service
  watchdog:
    build: ./watchdog
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./logs/watchdog:/var/log/watchdog
    environment:
      - MONITORED_CONTAINER=ollama-proxy-ollama-1
      - CHECK_INTERVAL=5
      - RESTART_COOLDOWN=60
      - LOG_LEVEL=INFO
    depends_on:
      - ollama
    networks:
      - ollama_network
    healthcheck:
      test: ["CMD", "/usr/local/bin/watchdog.sh", "healthcheck"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

volumes:
  ollama_data:

networks:
  ollama_network: